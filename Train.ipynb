{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dee0b8a2-cd27-43b3-a986-65945bf79400",
   "metadata": {},
   "source": [
    "# Setting up the background_sound folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f49b07-2767-4052-9350-751ef969d01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import models, layers\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import tensorflow as tf  # Import TensorFlow for TFLite conversion\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb266866-a54f-45bb-954a-84c1e41e2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to record audio with a Wake Word, saving the recordings to the specified path\n",
    "def record_audio_and_save(save_path, n_times=50):\n",
    "    # Prompt user to start recording the Wake Word\n",
    "    input(\"To start recording Wake Word press Enter: \")\n",
    "    \n",
    "    # Loop to record the specified number of times (n_times)\n",
    "    for i in range(n_times):\n",
    "        fs = 44100  # Sampling frequency (samples per second)\n",
    "        seconds = 2  # Duration of each recording in seconds\n",
    "\n",
    "        # Record audio for the specified duration and channels (stereo)\n",
    "        myrecording = sd.rec(int(seconds * fs), samplerate=fs, channels=2)\n",
    "        sd.wait()  # Wait until recording is finished\n",
    "        \n",
    "        # Save the recording to the given path with a numbered filename\n",
    "        write(save_path + str(i) + \".wav\", fs, myrecording)\n",
    "        \n",
    "        # Prompt to either proceed to next recording or stop the loop\n",
    "        input(f\"Press Enter to record next or stop with ctrl + C ({i + 1}/{n_times}): \")\n",
    "\n",
    "# Function to record background sounds, saving them to the specified path\n",
    "def record_background_sound(save_path, n_times=50):\n",
    "    # Prompt user to start recording background sounds\n",
    "    input(\"To start recording your background sounds press Enter: \")\n",
    "    \n",
    "    # Loop to record the specified number of times (n_times)\n",
    "    for i in range(n_times):\n",
    "        fs = 44100  # Sampling frequency (samples per second)\n",
    "        seconds = 2  # Duration of each recording in seconds\n",
    "\n",
    "        # Record audio for the specified duration and channels (stereo)\n",
    "        myrecording = sd.rec(int(seconds * fs), samplerate=fs, channels=2)\n",
    "        sd.wait()  # Wait until recording is finished\n",
    "        \n",
    "        # Save the background sound recording to the specified path with a numbered filename\n",
    "        write(save_path + str(i) + \".wav\", fs, myrecording)\n",
    "        \n",
    "        # Provide feedback on the progress of recording\n",
    "        print(f\"Currently on {i + 1}/{n_times}\")\n",
    "\n",
    "# Step 1: Record yourself saying the Wake Word\n",
    "print(\"Recording the Wake Word:\\n\")\n",
    "record_audio_and_save(\"WakeWordDetection/audio_data\", n_times=100)  # Save to specified folder with 100 recordings\n",
    "\n",
    "# Step 2: Record background sounds (Just let it run, it will automatically record)\n",
    "print(\"Recording the Background sounds:\\n\")\n",
    "record_background_sound(\"background_sound/\", n_times=100)  # Save to specified folder with 100 recordings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55401b77-bf6f-4dea-9b79-6a4b719800f2",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31223f47-a7c2-464f-8269-ada8c50e4361",
   "metadata": {},
   "source": [
    "### LOADING THE VOICE DATA FOR VISUALIZATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7059514f-df95-4b7f-85b7-7f8d885bce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "walley_sample = \"audio_data/0.wav\"  # Path to the audio file to be loaded\n",
    "# Load the audio data using librosa, the sample rate (sampling frequency) is also returned\n",
    "data, sample_rate = librosa.load(walley_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c00be9f-2ed5-4b1a-b259-06f838e8aaf3",
   "metadata": {},
   "source": [
    "### VISUALIZING WAVE FORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98981fa9-8a7a-4951-8732-6837ab9ead93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Wave Form\")  # Set the title of the plot\n",
    "# Use librosa to display the waveform of the loaded audio data\n",
    "librosa.display.waveshow(data, sr=sample_rate)\n",
    "plt.show()  # Show the waveform plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf99270-9ce9-44d9-913f-db64ce8777bf",
   "metadata": {},
   "source": [
    "### VISUALIZING MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6073b21f-cb48-40d9-8a3c-0118e166395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Mel-Frequency Cepstral Coefficients (MFCC) from the audio data\n",
    "# MFCCs are features that represent the short-term power spectrum of a sound, commonly used in speech processing\n",
    "mfccs = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=40)  # Extract 40 MFCCs\n",
    "print(\"Shape of mfcc:\", mfccs.shape)  # Print the shape of the MFCC matrix to check its dimensions\n",
    "\n",
    "# Plot the MFCCs as a spectrogram\n",
    "plt.title(\"MFCC\")  # Set the title of the plot\n",
    "# Display the MFCCs over time using librosaâ€™s specshow function\n",
    "librosa.display.specshow(mfccs, sr=sample_rate, x_axis='time')\n",
    "plt.show()  # Show the MFCC plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85333fc-21f0-40b0-9f7a-61e57ad741b5",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b36c92-ebbe-4084-a91d-29f1bde26036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "fs = 44100  # Sampling frequency for audio recordings (samples per second)\n",
    "seconds = 2  # Duration of each audio clip in seconds\n",
    "filename = \"prediction.wav\"  # Name of the prediction audio file\n",
    "class_names = [\"Wake Word NOT Detected\", \"Wake Word Detected\"]  # Class labels for the detection task\n",
    "num_labels = 2  # Number of classes (Wake Word Detected or Not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7fb2bf-f460-4ddc-a889-3be34c242e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input shape for the Conv2D model\n",
    "input_shape = (32, 32, 1)  # Input shape: Height, Width, Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85b467b-27f0-48ef-912d-197415b31749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio data and preprocess\n",
    "all_data = []  # List to store the features and labels\n",
    "data_path_dict = {\n",
    "    0: [\"background_sound/\" + file_path for file_path in os.listdir(\"background_sound/\")],  # Background sounds\n",
    "    1: [\"audio_data/\" + file_path for file_path in os.listdir(\"audio_data/\")]  # Wake Word sounds\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b4fb42-58c2-4dc0-93f8-18e171e760b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the two class labels (background sounds and wake word sounds)\n",
    "for class_label, list_of_files in data_path_dict.items():\n",
    "    for single_file in list_of_files:\n",
    "        # Load the audio file using librosa\n",
    "        audio, sample_rate = librosa.load(single_file)\n",
    "        \n",
    "        # Extract MFCC (Mel-Frequency Cepstral Coefficients) from the audio\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfcc_processed = np.mean(mfcc.T, axis=0)  # Take the mean of the MFCC over time\n",
    "        # Reshape for Conv2D input (40 MFCCs, 1 channel)\n",
    "        mfcc_processed = mfcc_processed.reshape(40, 1)  # Reshaped to (40, 1)\n",
    "        \n",
    "        # Append the processed MFCC and its corresponding class label\n",
    "        all_data.append([mfcc_processed, class_label])\n",
    "    \n",
    "    print(f\"Info: Successfully Preprocessed Class Label {class_label}\")  # Print info after processing each class\n",
    "\n",
    "# Create a DataFrame from the preprocessed data\n",
    "df = pd.DataFrame(all_data, columns=[\"feature\", \"class_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7291e2b-3c58-4f23-90c5-2c82548d48f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making our data training-ready \n",
    "X = df[\"feature\"].values  # Extract the feature column\n",
    "\n",
    "# Create an array to hold the padded features (to match the required input shape for Conv2D)\n",
    "padded_features = []\n",
    "\n",
    "# Loop through each feature and pad or resize to (32, 32)\n",
    "for feature in X:\n",
    "    # Pad or resize each feature to a shape of (32, 32)\n",
    "    feature = np.pad(feature, ((0, max(0, 32 - feature.shape[0])), (0, 31)), 'constant')\n",
    "    feature = feature[:32]  # Take the first 32 rows if the feature is too large\n",
    "    padded_features.append(feature)\n",
    "\n",
    "X = np.array(padded_features)  # Convert the padded features to a numpy array\n",
    "X = np.reshape(X, (-1, 32, 32, 1))  # Reshape to fit Conv2D input shape: (32, 32, 1)\n",
    "\n",
    "# Convert the class labels to one-hot encoding\n",
    "y = np.array(df[\"class_label\"].tolist())\n",
    "y = to_categorical(y)  # One-hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeb177c-b939-43e0-bffd-9924732c7e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split \n",
    "# Split the data into training and test sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Number of elements in X_train:\", len(X_train))\n",
    "print(\"Number of elements in X_test:\", len(X_test))\n",
    "print(\"Number of elements in y_train:\", len(y_train))\n",
    "print(\"Number of elements in y_test:\", len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134a7ccf-ba65-4e2e-bcac-068741cca1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training #######\n",
    "# Define the neural network model using Keras Sequential API\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),  # Input layer with shape (32, 32, 1)\n",
    "    layers.Resizing(32, 32),  # Resize input to (32, 32) if needed\n",
    "    layers.Conv2D(32, 3, activation='relu'),  # Convolutional layer with 32 filters, 3x3 kernel\n",
    "    layers.Conv2D(64, 3, activation='relu'),  # Another convolutional layer with 64 filters\n",
    "    layers.MaxPooling2D(),  # Max pooling to reduce spatial dimensions\n",
    "    layers.Dropout(0.25),  # Dropout layer to prevent overfitting\n",
    "    layers.Flatten(),  # Flatten the output for the fully connected layer\n",
    "    layers.Dense(128, activation='relu'),  # Dense layer with 128 neurons\n",
    "    layers.Dropout(0.5),  # Dropout layer for regularization\n",
    "    layers.Dense(num_labels, activation='softmax')  # Output layer with softmax activation for classification\n",
    "])\n",
    "\n",
    "print(model.summary())  # Print the summary of the model architecture\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",  # Loss function for multi-class classification\n",
    "    optimizer='adam',  # Adam optimizer\n",
    "    metrics=['accuracy']  # Accuracy as the evaluation metric\n",
    ")\n",
    "\n",
    "# Train the model for 150 epochs with a validation split of 0.2\n",
    "print(\"Model Score: \\n\")\n",
    "history = model.fit(X_train, y_train, epochs=150, validation_split=0.2)\n",
    "model.save(\"saved_model/WWD.h5\")  # Save the trained model\n",
    "score = model.evaluate(X_test, y_test)  # Evaluate the model on the test set\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f37a1e0-fbe2-49d7-bee0-0816f2ce8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Keras model to TFLite \n",
    "# Convert the trained Keras model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model to a file\n",
    "tflite_model_path = \"Untitled Folder/WWD.tflite\"\n",
    "with open(tflite_model_path, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"TFLite model conversion completed and saved at\", tflite_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d89f4a2-219c-4e48-82cd-35d522d84bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate TFLite model \n",
    "# Load the TFLite model and allocate tensors for inference\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the TFLite model on the test data\n",
    "y_pred_tflite = []\n",
    "for i in range(len(X_test)):\n",
    "    # Prepare input data and run inference on TFLite model\n",
    "    input_data = np.expand_dims(X_test[i], axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    y_pred_tflite.append(np.argmax(output_data))  # Get the predicted class\n",
    "\n",
    "# Display the classification report for the TFLite model\n",
    "print(\"TFLite Model Classification Report: \\n\")\n",
    "cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred_tflite)\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred_tflite))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
